{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T00:51:04.767798Z","iopub.execute_input":"2023-09-21T00:51:04.768718Z","iopub.status.idle":"2023-09-21T00:51:05.245538Z","shell.execute_reply.started":"2023-09-21T00:51:04.768675Z","shell.execute_reply":"2023-09-21T00:51:05.243906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 导入必要的库","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom colorama import Fore, Style, init\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom scipy import stats\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:05.247989Z","iopub.execute_input":"2023-09-21T00:51:05.248558Z","iopub.status.idle":"2023-09-21T00:51:07.792388Z","shell.execute_reply.started":"2023-09-21T00:51:05.248519Z","shell.execute_reply":"2023-09-21T00:51:07.790816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 读取数据","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv', index_col = 'id')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv', index_col = 'id')\noriginal_df = pd.read_csv('/kaggle/input/horse-survival-dataset/horse.csv')\ntrain_df.index.name = None\ntest_df.index.name = None\n\noriginal_df.index = range(len(original_df))\noriginal_df.index += max(test_df.index) + 1\noriginal_df = original_df.reindex(train_df.columns, axis=1)\n\n# 融合的数据\ntrainANDoriginal_df = pd.concat([train_df, original_df], axis=0, ignore_index = True)\n\nfeature = train_df.columns[:-1]\ntarget = train_df.columns[-1]\ntarget_map = {\"lived\":1,\"died\":2,\"euthanized\":3}\ntrain_df[target] = train_df[target].map(target_map).astype(np.int8)\noriginal_df[target] = original_df[target].map(target_map).astype(np.int8)\ntrainANDoriginal_df[target] = trainANDoriginal_df[target].map(target_map).astype(np.int8)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:07.794709Z","iopub.execute_input":"2023-09-21T00:51:07.795146Z","iopub.status.idle":"2023-09-21T00:51:07.906488Z","shell.execute_reply.started":"2023-09-21T00:51:07.795106Z","shell.execute_reply":"2023-09-21T00:51:07.905070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## datadict","metadata":{}},{"cell_type":"code","source":"# with open('/kaggle/input/horse-colic/datadict.txt', 'r') as file:\n#    for line in file:\n#        print(line)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:07.911762Z","iopub.execute_input":"2023-09-21T00:51:07.912507Z","iopub.status.idle":"2023-09-21T00:51:07.919161Z","shell.execute_reply.started":"2023-09-21T00:51:07.912464Z","shell.execute_reply":"2023-09-21T00:51:07.917698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 常用函数","metadata":{}},{"cell_type":"code","source":"def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    print(style + color + text + style)\n\n# 字典编码函数  排序映射赋值（有序离散object处理）\ndef change_object_cols(se):\n    value = se.unique().tolist()\n    value.sort()\n    return se.map(pd.Series(range(len(value),index=value))).values\n# 多变量分析  将两个变量融合进而分析\ndef combine_feature(df):\n    cols = df.columns\n    feature1 = df[cols[0]].astype(str).values.tolist()\n    feature2 = df[cols[1]].astype(str).values.tolist()\n    return pd.Series([feature1[i] + '&' + feature2[i] for i in range(df.shape[0])])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:07.920724Z","iopub.execute_input":"2023-09-21T00:51:07.921600Z","iopub.status.idle":"2023-09-21T00:51:07.938711Z","shell.execute_reply.started":"2023-09-21T00:51:07.921563Z","shell.execute_reply":"2023-09-21T00:51:07.937104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 对抗验证 adversarial CV","metadata":{}},{"cell_type":"code","source":"# # Performing adversarial CV between the 2 specified datasets:-\n# def Do_AdvCV(df1:pd.DataFrame, df2:pd.DataFrame, source1:str, source2:str):\n#     \"This function performs an adversarial CV between the 2 provided datasets if needed by the user\";\n    \n#     # Adversarial CV per column:-\n#     ftre = pp.test.select_dtypes(include = np.number).\\\n#     drop(columns = ['id', \"Source\"], errors = 'ignore').columns;\n#     adv_cv = {};\n\n#     for col in ftre:\n#         shuffle_state = np.random.randint(low = 10, high = 100, size= 1);\n\n#         full_df = \\\n#         pd.concat([df1[[col]].assign(Source = source1), df2[[col]].assign(Source = source2)], \n#                   axis=0, ignore_index = True).\\\n#         sample(frac = 1.00, random_state = shuffle_state);\n\n#         full_df = full_df.assign(Source_Nb = full_df['Source'].eq(source2).astype(np.int8));\n\n#         # Checking for adversarial CV:-\n#         model = LGBMClassifier(random_state = CFG.state, max_depth = 6, learning_rate = 0.05);\n#         cv    = all_cv['SKF'];\n#         score = np.mean(cross_val_score(model, \n#                                         full_df[[col]], \n#                                         full_df.Source_Nb, \n#                                         scoring= 'roc_auc', \n#                                         cv     = cv)\n#                        );\n#         adv_cv.update({col: round(score, 4)});\n#         collect();\n    \n#     del ftre;\n#     collect();\n    \n#     fig, ax = plt.subplots(1,1,figsize = (12, 5));\n#     pd.Series(adv_cv).plot.bar(color = 'tab:blue', ax = ax);\n#     ax.axhline(y = 0.60, color = 'red', linewidth = 2.75);\n#     ax.grid(**CFG.grid_specs); \n#     plt.yticks(np.arange(0.0, 0.81, 0.05));\n#     plt.show();\n    \n# # Implementing the adversarial CV:-\n# if CFG.adv_cv_req == \"Y\":\n#     PrintColor(f\"\\n---------- Adversarial CV - Train vs Original ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.train, df2 = pp.original, source1 = 'Train', source2 = 'Original');\n    \n#     PrintColor(f\"\\n---------- Adversarial CV - Train vs Test ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.train, df2 = pp.test, source1 = 'Train', source2 = 'Test');\n    \n#     PrintColor(f\"\\n---------- Adversarial CV - Original vs Test ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.original, df2 = pp.test, source1 = 'Original', source2 = 'Test');   \n    \n# if CFG.adv_cv_req == \"N\":\n#     PrintColor(f\"\\nAdversarial CV is not needed\\n\", color = Fore.RED);\n    \n# collect();\n# print();","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:07.940928Z","iopub.execute_input":"2023-09-21T00:51:07.941485Z","iopub.status.idle":"2023-09-21T00:51:07.962769Z","shell.execute_reply.started":"2023-09-21T00:51:07.941440Z","shell.execute_reply":"2023-09-21T00:51:07.960875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Train\nPrintColor(f\"\\nTrain set head\", color = Fore.BLUE)\ndisplay(train_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTrain set description\", color = Fore.BLUE)\ndisplay(train_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTrain set Information\", color = Fore.BLUE)\ndisplay(train_df.info())\n\n# Original\nPrintColor(f\"\\nOriginal set head\", color = Fore.BLUE)\ndisplay(original_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nOriginal set description\", color = Fore.BLUE)\ndisplay(original_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nOriginal set Information\", color = Fore.BLUE)\ndisplay(original_df.info())\n\n# TrainANDoriginal\nPrintColor(f\"\\nTrainANDoriginal set head\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTrainANDoriginal set description\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTrainANDoriginal set Information\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.info())\n\n# Test\nPrintColor(f\"\\nTest set head\", color = Fore.BLUE)\ndisplay(test_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTest set description\", color = Fore.BLUE)\ndisplay(test_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTest set Information\", color = Fore.BLUE)\ndisplay(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:07.965319Z","iopub.execute_input":"2023-09-21T00:51:07.965841Z","iopub.status.idle":"2023-09-21T00:51:08.473376Z","shell.execute_reply.started":"2023-09-21T00:51:07.965798Z","shell.execute_reply":"2023-09-21T00:51:08.471813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.hospital_number 有重复、出现数值很大的编号、是否应该当成离散量重新编一下码----作为类别变量----独热编码\n##   hospital_number 在test中出现但在train中没出现 ---- 需要test和train合并编码\n## 2.lesion_1、lesion_2、lesion_3数据比较有问题，需要如何处理\n##  2.1lesion_2为非0，outcome为非died\n##     可处理df['lesion_2'] = df['lesion_2'].apply(lambda x:1 if x>0 else 0)  （预测died变好？）\n##     作为类别变量处理\n##  2.2lesion_3只有两个值，lession_3非0，outcome非died\n##     同上处理？\n##     作为类别变量处理\n##  2.3查看到其特征解释貌似可以拆解开分为多个特征\n##     见datadict","metadata":{}},{"cell_type":"markdown","source":"## other tricks\n## 1.马儿的温度影响outcome，偏离37.8越多越容易死\n##   df[\"deviation_from_normal_temp\"] = df[\"rectal_temp\"].apply(lambda x: abs(x - 37.8))\n## 2.pulse, respiatory_rate, packed_cell_volume是否可以如1","metadata":{}},{"cell_type":"code","source":"# 数据正确性验证\n# 缺失值\n# 异常值\n# 规律性分析 （单变量分析、多变量分析）\n# 离散型变量区分、名义型变量（男女-10）、有序（考虑是否当成连续）","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:08.475409Z","iopub.execute_input":"2023-09-21T00:51:08.476908Z","iopub.status.idle":"2023-09-21T00:51:08.482513Z","shell.execute_reply.started":"2023-09-21T00:51:08.476861Z","shell.execute_reply":"2023-09-21T00:51:08.481324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 缺失数据分析1\nPrintColor(f\"\\nTrain set null values\", color = Fore.BLUE)\ntrain_missing = train_df.isnull().sum() * 100 / len(train_df)\ntrain_missing = train_missing[train_missing != 0]\ntrain_missing = pd.DataFrame({'missing percent': train_missing})\ntrain_missing = train_missing.sort_values('missing percent', ascending=False)\ndisplay(train_missing)\nfor col in train_missing.index:\n    print(col,train_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nOriginal set null values\", color = Fore.BLUE)\noriginal_missing = original_df.isnull().sum() * 100 / len(original_df)\noriginal_missing = original_missing[original_missing != 0]\noriginal_missing = pd.DataFrame({'missing percent': original_missing})\noriginal_missing = original_missing.sort_values('missing percent', ascending=False)\ndisplay(original_missing)\nfor col in original_missing.index:\n    print(col,original_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nTrainAndOriginal set null values\", color = Fore.BLUE)\ntrainANDoriginal_missing = trainANDoriginal_df.isnull().sum() * 100 / len(trainANDoriginal_df)\ntrainANDoriginal_missing = trainANDoriginal_missing[trainANDoriginal_missing != 0]\ntrainANDoriginal_missing = pd.DataFrame({'missing percent': trainANDoriginal_missing})\ntrainANDoriginal_missing = trainANDoriginal_missing.sort_values('missing percent', ascending=False)\ndisplay(trainANDoriginal_missing)\nfor col in trainANDoriginal_missing.index:\n    print(col,trainANDoriginal_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nTest set null values\", color = Fore.BLUE)\ntest_missing = test_df.isnull().sum() * 100 / len(test_df)\ntest_missing = test_missing[test_missing != 0]\ntest_missing = pd.DataFrame({'missing percent': test_missing})\ntest_missing = test_missing.sort_values('missing percent', ascending=False)\ndisplay(test_missing)\nfor col in test_missing.index:\n    print(col,test_df[col].unique())#此处缺失值如何去补充","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:08.484714Z","iopub.execute_input":"2023-09-21T00:51:08.485560Z","iopub.status.idle":"2023-09-21T00:51:08.569782Z","shell.execute_reply.started":"2023-09-21T00:51:08.485515Z","shell.execute_reply":"2023-09-21T00:51:08.568474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 缺失数据分析2\ndef summary_df(train_df,original_df,trainANDoriginal_df,test_df):\n    summary = pd.DataFrame(train_df.dtypes, columns=['dtypes'])\n    summary['train_missing#'] = train_df.isna().sum()\n    summary['train_missing%'] = (train_df.isna().sum())/len(train_df)\n    summary['train_uniques'] = train_df.nunique().values\n    summary['train_count'] = train_df.count().values\n    # summary['train_skew'] = train_df.skew().values\n    summary['original_missing#'] = original_df.isna().sum()\n    summary['original_missing%'] = (original_df.isna().sum())/len(original_df)\n    summary['original_uniques'] = original_df.nunique().values\n    summary['original_count'] = original_df.count().values\n    # summary['original_skew'] = original_df.skew().values\n    summary['test_missing#'] = test_df.isna().sum()\n    summary['test_missing%'] = (test_df.isna().sum())/len(test_df)\n    summary['test_uniques'] = test_df.nunique().values\n    summary['test_count'] = test_df.count().values\n    # summary['test_skew'] = train_df.skew().values\n    summary['trainANDoriginal_missing#'] = trainANDoriginal_df.isna().sum()\n    summary['trainANDoriginal_missing%'] = (trainANDoriginal_df.isna().sum())/len(trainANDoriginal_df)\n    summary['trainANDoriginal_uniques'] = trainANDoriginal_df.nunique().values\n    summary['trainANDoriginal_count'] = trainANDoriginal_df.count().values\n    # summary['trainANDoriginal_skew'] = trainANDoriginal_df.skew().values\n    return summary\nsummary_df(train_df[feature],original_df[feature],trainANDoriginal_df[feature],test_df[feature]).style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:08.575574Z","iopub.execute_input":"2023-09-21T00:51:08.576088Z","iopub.status.idle":"2023-09-21T00:51:08.701503Z","shell.execute_reply.started":"2023-09-21T00:51:08.576047Z","shell.execute_reply":"2023-09-21T00:51:08.699823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.缺失值数据均为object类型\n## 2.缺失值数据中的nan，有些还有none的存在，应该如何处理nan。  检查了一下数据发现是大写的None和小写的none的问题？\n##   好像两者是不同的\n## 3.","metadata":{}},{"cell_type":"code","source":"# 寻找object类型、或者直接规定\ndtypes = train_df.dtypes.to_dict()\ncat_cols = []\nnum_cols = []\nfor column, typ in dtypes.items():\n    if typ == \"object\":\n        cat_cols.append(column)\n    else:\n        num_cols.append(column)\n\n# 独特性验证(类别类、独特类（id等）)\nfor col in cat_cols:\n    print('Train   ',col,train_df[col].unique())\n    print('Original   ',col,original_df[col].unique())\n    if col==target:\n        continue\n    print('Test   ',col,test_df[col].unique())\n\ndef unique_validation(train_df,test_df,original_df,trainANDoriginal_df,features):\n    for feature in features:\n        print('train',feature,train_df[feature].unique())\n        print('test',feature,test_df[feature].unique())\n        print('original',feature,original_df[feature].unique())\n        print('trainANDoriginal',feature,trainANDoriginal_df[feature].unique())\n\n        print('original_unique_to_train',feature,[original_unique_to_train for original_unique_to_train in original_df[feature].unique() if original_unique_to_train not in train_df[feature].unique()])\n        print('test_unique_to_train',feature,[test_unique_to_train for test_unique_to_train in test_df[feature].unique() if test_unique_to_train not in train_df[feature].unique()])\n        print('train_unique_to_original',feature,[train_unique_to_original for train_unique_to_original in train_df[feature].unique() if train_unique_to_original not in original_df[feature].unique()])\n        print('train_unique_to_test',feature,[train_unique_to_test for train_unique_to_test in train_df[feature].unique() if train_unique_to_test not in test_df[feature].unique()])\n        print('original_unique_to_test',feature,[original_unique_to_test for original_unique_to_test in original_df[feature].unique() if original_unique_to_test not in test_df[feature].unique()])\n        print('test_unique_to_original',feature,[test_unique_to_original for test_unique_to_original in test_df[feature].unique() if test_unique_to_original not in original_df[feature].unique()])\n\n        print('trainANDoriginal_unique_to_test',feature,[trainANDoriginal_unique_to_test for trainANDoriginal_unique_to_test in trainANDoriginal_df[feature].unique() if trainANDoriginal_unique_to_test not in test_df[feature].unique()])\n        print('test_unique_to_trainANDoriginal',feature,[test_unique_to_trainANDoriginal for test_unique_to_trainANDoriginal in test_df[feature].unique() if test_unique_to_trainANDoriginal not in trainANDoriginal_df[feature].unique()])\n\nunique_validation(train_df,test_df,original_df,trainANDoriginal_df,['lesion_1','lesion_2','lesion_3'])#,'hospital_number'","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:08.703993Z","iopub.execute_input":"2023-09-21T00:51:08.704557Z","iopub.status.idle":"2023-09-21T00:51:08.775412Z","shell.execute_reply.started":"2023-09-21T00:51:08.704510Z","shell.execute_reply":"2023-09-21T00:51:08.773787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#单变量分布1\nfig, axes = plt.subplots(len(cat_cols), 4, figsize = (20, len(cat_cols)* 2), \n                             gridspec_kw = {'wspace': 0.35, 'hspace': 0.5})\nfor i, column in enumerate(cat_cols):\n    ax1 = axes[i, 0]\n    ax2 = axes[i, 1]\n    ax3 = axes[i, 2]\n    ax4 = axes[i, 3]\n    a = train_df[column].value_counts(normalize = True)\n    a.sort_index().plot.barh(ax = ax1, color = 'purple')\n    ax1.set_title(f\"{column}_Train\")\n    ax1.set(ylabel = '')\n    b = original_df[column].value_counts(normalize = True)\n    b.sort_index().plot.barh(ax = ax2, color = 'purple')\n    ax2.set_title(f\"{column}_Original\")\n    ax2.set(ylabel = '')\n    d = trainANDoriginal_df[column].value_counts(normalize = True)\n    d.sort_index().plot.barh(ax = ax4, color = 'purple')\n    ax4.set_title(f\"{column}_TrainAndOriginal\")\n    ax4.set(ylabel = '')\n    if column==target:\n        continue\n    c = test_df[column].value_counts(normalize = True)\n    c.sort_index().plot.barh(ax = ax3, color = 'purple')\n    ax3.set_title(f\"{column}_Test\")\n    ax3.set(ylabel = '')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:08.777479Z","iopub.execute_input":"2023-09-21T00:51:08.778236Z","iopub.status.idle":"2023-09-21T00:51:19.941115Z","shell.execute_reply.started":"2023-09-21T00:51:08.778184Z","shell.execute_reply":"2023-09-21T00:51:19.940178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.pain  训练集slight  测试集moderate\n##   考虑两者合并之后编码\n## 2.peristalsis 训练集多一个distend_small（1个样本）\n## 3.nasogastric_reflux 训练集多一个slight（1个样本）\n## 4.rectal_exam_feces 训练集多一个serosanguious（1个样本）\n##   234不删除可能也不影响，删除好像也不影响\n## 5.mucous 中的pale_cyanotic分布不太一样（异常突出）\n## 6.abdominal_distention、nasogastric_tube、nasogastric_reflux中的none   \n##   考虑将None和none 都做none（普通字符串none的意义）处理？\n##  （用isnull统计的None被视为无值，但是none被视为普通字符串）\n##   此处处理完之后还需要做一下训练集和测试集分布分析\n","metadata":{}},{"cell_type":"code","source":"# 单变量分布\nfig, axes = plt.subplots(len(num_cols), 5, figsize = (20, len(num_cols)* 4), \n                             gridspec_kw = {'wspace': 0.35, 'hspace': 0.5, 'width_ratios': [0.80,0.20, 0.20, 0.20, 0.20]})\nfor i, column in enumerate(num_cols):\n    ax = axes[i,0]\n    a = train_df[column]\n    a.plot(ax = ax,kind='density', color = 'purple',label='Train')\n    b = original_df[column]\n    b.plot(ax = ax,kind='density', color = 'pink',label='Original')\n    if column not in ['lesion_3','outcome']:\n        c = test_df[column]\n        c.plot(ax = ax,kind='density', color = 'red',label='Test')\n    d = trainANDoriginal_df[column]\n    d.plot(ax = ax,kind='density', color = 'green',label='TrAndOr')\n    ax.set_title(f\"{column}_Train_vs_Test_vs_Original_vs_TrAndOr\")\n    ax.set(ylabel = '')\n    ax.legend(['Train','Original','Test','TrAndOr'])\n     \n    \n    ax = axes[i,1]\n    sns.boxplot(y = a, width = 0.25,saturation = 0.90, linewidth = 0.90, fliersize= 2.25, color = '#037d97',ax = ax)\n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"Train\",fontsize = 9, fontweight= 'bold')\n    ax = axes[i,2]\n    sns.boxplot(y = b, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#E4591E',ax = ax) \n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"Original\",fontsize = 9, fontweight= 'bold')\n    ax = axes[i,4]\n    sns.boxplot(y = d, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#B78C25',ax = ax) \n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"TrainAndOriginal\",fontsize = 9, fontweight= 'bold')\n    if column not in ['lesion_3','outcome']:\n        ax = axes[i,3]\n        sns.boxplot(y = c, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#544E91',ax = ax)\n        ax.set(xlabel = '', ylabel = '')\n        ax.set_title(f\"Test\",fontsize = 9, fontweight= 'bold')\n# 多变量分析","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:19.942246Z","iopub.execute_input":"2023-09-21T00:51:19.943229Z","iopub.status.idle":"2023-09-21T00:51:33.291732Z","shell.execute_reply.started":"2023-09-21T00:51:19.943193Z","shell.execute_reply":"2023-09-21T00:51:33.290400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.单变量分析，训练集和测试集分布基本一致，除了hospital_number和lesion_3差别\n## 2.hospital_number\n## 3.lesion_3","metadata":{}},{"cell_type":"code","source":"# 联合outcome的单变量分布\ndef plot_count(train_df,original_df,trainANDoriginal_df,columns,hue,other_features=['lesion_1','lesion_2','lesion_3','hospital_number']):\n    columns = columns + other_features\n    n_rows = len(columns)\n    fig, axes = plt.subplots(n_rows, 3, figsize=(17, 4 * n_rows))\n    for i, column in enumerate(columns):\n        ax = axes[i,0]\n        sns.countplot(data=train_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'Tarin_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,1]\n        sns.countplot(data=original_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'Original_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,2]\n        sns.countplot(data=trainANDoriginal_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'TarinAndOriginal_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n\n    ylim_top = ax.get_ylim()[1]\n    ax.set_ylim(top=ylim_top * 1.1)\n    for i in range(len(columns), len(axes)):\n        axes[i,0].axis('off')\n        axes[i,1].axis('off')\n        axes[i,2].axis('off')\n\n    # fig.suptitle(plotname, fontsize=25, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \nplot_count(train_df,original_df,trainANDoriginal_df,[cat_col for cat_col in cat_cols if cat_col != target],target,other_features=['lesion_2','lesion_3'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:33.293853Z","iopub.execute_input":"2023-09-21T00:51:33.294264Z","iopub.status.idle":"2023-09-21T00:51:50.127474Z","shell.execute_reply.started":"2023-09-21T00:51:33.294230Z","shell.execute_reply":"2023-09-21T00:51:50.126415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 双变量配对分析\ndef plot_pair(train_df,num_var,target,plotname):\n    g = sns.pairplot(data=train_df, x_vars=num_var, y_vars=num_var, hue=target, corner=True)\n    g._legend.set_bbox_to_anchor((0.8, 0.7))\n    g._legend.set_title(target)\n    g._legend.loc = 'upper center'\n    g._legend.get_title().set_fontsize(14)\n    for item in g._legend.get_texts():\n        item.set_fontsize(14)\n\n    plt.suptitle(plotname, ha='center', fontweight='bold', fontsize=25, y=0.98)\n    plt.show()\n\n# plot_pair(train_df,num_cols,target,plotname = 'Scatter Matrix with Target Of Train')\n# plt.tight_layout()\n# plot_pair(original_df,num_cols,target,plotname = 'Scatter Matrix with Target Of Original')\n# plt.tight_layout()\n# plot_pair(trainANDoriginal_df,num_cols,target,plotname = 'Scatter Matrix with Target Of TrainAndOriginal')\n# plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:50.128675Z","iopub.execute_input":"2023-09-21T00:51:50.129443Z","iopub.status.idle":"2023-09-21T00:51:50.137514Z","shell.execute_reply.started":"2023-09-21T00:51:50.129397Z","shell.execute_reply":"2023-09-21T00:51:50.136414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"method = \"pearson\"\nplt.figure(figsize=(15,15))\ncorr = train_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_train\")\n\nplt.figure(figsize=(15,15))\ncorr = original_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_original\")\n\nplt.figure(figsize=(15,15))\ncorr = test_df[[num_col for num_col in num_cols if num_col != target]].corr(method = \"pearson\")# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_test\")\n\nplt.figure(figsize=(15,15))\ncorr = trainANDoriginal_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_trainANDoriginal\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:50.138892Z","iopub.execute_input":"2023-09-21T00:51:50.139208Z","iopub.status.idle":"2023-09-21T00:51:53.225973Z","shell.execute_reply.started":"2023-09-21T00:51:50.139183Z","shell.execute_reply":"2023-09-21T00:51:53.224685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.pulse 和 respiratory_rate、packed_cell_volume正相关比较明显\n## 2.nasogastric_reflux_ph 和total_protein、abdomo_protein分别呈负相关、正相关\n## 3.total_protein和abdomo_protein呈正相关\n## 4.lesion_2和lesion_3呈正相关","metadata":{}},{"cell_type":"markdown","source":"## 卡方检验","metadata":{}},{"cell_type":"code","source":"def chi_squared_test(df, input_var, target_var, significance_level=0.05):\n    contingency_table = pd.crosstab(df[input_var], df[target_var])\n    chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n    \n    if p < significance_level:\n        print(f'\\033[32m{input_var} has a significant relationship with the target variable.\\033[0m') \n    else:\n        print(f'\\033[31m{input_var} does not have a significant relationship with the target variable.\\033[0m')  \n\nfor cat_col in cat_cols:\n    chi_squared_test(train_df, cat_col, target)\n    chi_squared_test(original_df, cat_col, target)\n    chi_squared_test(trainANDoriginal_df, cat_col, target)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.227765Z","iopub.execute_input":"2023-09-21T00:51:53.228553Z","iopub.status.idle":"2023-09-21T00:51:53.611813Z","shell.execute_reply.started":"2023-09-21T00:51:53.228507Z","shell.execute_reply":"2023-09-21T00:51:53.610386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **特征工程**","metadata":{}},{"cell_type":"code","source":"## 1.通用组合特征\n## 2.业务统计特征\n## 我们需要融合？\n## NLP特征池衍生（from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer）","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.613547Z","iopub.execute_input":"2023-09-21T00:51:53.613975Z","iopub.status.idle":"2023-09-21T00:51:53.619338Z","shell.execute_reply.started":"2023-09-21T00:51:53.613934Z","shell.execute_reply":"2023-09-21T00:51:53.618090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(num_cols,f'\\n',cat_cols)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.620821Z","iopub.execute_input":"2023-09-21T00:51:53.621158Z","iopub.status.idle":"2023-09-21T00:51:53.633931Z","shell.execute_reply.started":"2023-09-21T00:51:53.621129Z","shell.execute_reply":"2023-09-21T00:51:53.632487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **pipeline**","metadata":{}},{"cell_type":"code","source":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n#     ('std_scaler', StandardScaler()),\n])\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"NaN\")),\n    ('encoder', OrdinalEncoder())\n])\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_cols),\n    (\"cat\", cat_pipeline, cat_cols),\n])\ntrain_prepared = full_pipeline.fit_transform(train_df)\ndisplay(train_df)\ndisplay(pd.DataFrame(train_prepared))\n\n# 或者单独处理？\n# Scaling:-\n# class Scaler(TransformerMixin, BaseEstimator):\n#     \"\"\"\n#     This class aims to create scaling for the provided dataset\n#     \"\"\";\n    \n#     def __init__(self, scl_method: str, scale_req: str, scl_cols):\n#         self.scl_method = scl_method;\n#         self.scale_req  = scale_req;\n#         self.scl_cols   = scl_cols;\n        \n#     def fit(self,X, y=None, **params):\n#         \"This function calculates the train-set parameters for scaling\";\n        \n#         self.params          = X[self.scl_cols].describe(percentiles = [0.25, 0.50, 0.75]).drop(['count'], axis=0).T;\n#         self.params['iqr']   = self.params['75%'] - self.params['25%'];\n#         self.params['range'] = self.params['max'] - self.params['min'];\n        \n#         return self;\n    \n#     def transform(self,X, y=None, **params):  \n#         \"This function transform the relevant scaling columns\";\n        \n#         df = X.copy();\n#         if self.scale_req == \"Y\":\n#             if CFG.scl_method == \"Z\":\n#                 df[self.scl_cols] = (df[self.scl_cols].values - self.params['mean'].values) / self.params['std'].values;\n#             elif CFG.scl_method == \"Robust\":\n#                 df[self.scl_cols] = (df[self.scl_cols].values - self.params['50%'].values) / self.params['iqr'].values;\n#             elif CFG.scl_method == \"MinMax\":\n#                 df[self.scl_cols] = (df[self.scl_cols].values - self.params['min'].values) / self.params['range'].values;\n#         else:\n#             PrintColor(f\"Scaling is not needed\", color = Fore.RED);\n    \n#         return df;","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.635665Z","iopub.execute_input":"2023-09-21T00:51:53.636510Z","iopub.status.idle":"2023-09-21T00:51:53.728203Z","shell.execute_reply.started":"2023-09-21T00:51:53.636474Z","shell.execute_reply":"2023-09-21T00:51:53.726714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Xformer(TransformerMixin, BaseEstimator):\n#     def __init__(self, hnb_only_test:list, hnb_only_train: list): \n#         self.sec_ftre_req = CFG.sec_ftre_req;\n#         self.hnb_only_test = hnb_only_test;\n#         self.hnb_only_train = hnb_only_train;\n    \n#     def fit(self, X, y= None, **params):\n#         hnb_diff = \\\n#         {nb: abs(nb - base_val) for nb in X['hospital_number'].unique() for base_val in self.hnb_only_test};\n#         self.hnb_mapper = {self.hnb_only_test[0]: min(hnb_diff, key= hnb_diff.get)}\n    \n#         self.ip_cols = X.columns;\n#         return self;\n    \n#     def transform(self, X, y= None, **params):       \n#         global strt_ftre;\n#         df    = X.copy();  \n        \n#         # Correcting the test-only hospital number with the nearest training set number:-\n#         try:\n#             df['hospital_number'] = df['hospital_number'].astype(np.int64).map(self.hnb_mapper).astype(np.int64);\n#         except:\n#             PrintColor(f\"---> Check the hospital number assignment\", Fore.RED);\n#             df.loc[df['hospital_number'] == 528338, \"hospital_number\"] = 528355;\n        \n#         # Splitting the lesion 1 feature into its components:-\n#         l1_prf = \\\n#         df['lesion_1'].astype(np.int64).\\\n#         astype(str).str.split('', expand = True).\\\n#         applymap(lambda x: 0 if x in ['', None, np.NaN] else x).astype(np.int8).\\\n#         add_prefix(\"lesion_1_\").\\\n#         drop(columns = ['lesion_1_0', \"lesion_1_6\"], errors = \"ignore\");\n        \n#         df = pd.concat([df, l1_prf], axis=1);\n#         del l1_prf;\n\n#         if self.sec_ftre_req == \"Y\":\n#             df['rectal_temp_risk'] = np.where(df.rectal_temp >= 37.8,1,0).astype(np.int8);\n#             df['pulse_risk']       = np.where(df.pulse >= 40,1,0).astype(np.int8);\n#             df['cell_vol_risk']    = np.where(df.packed_cell_volume >= 50, 1,0).astype(np.int8);\n#             df['protein_risk']     = np.where(df.total_protein >= 7.5, 1,0).astype(np.int8);\n                \n#         if CFG.sec_ftre_req != \"Y\": \n#             PrintColor(f\"Secondary features are not required\", color = Fore.RED);    \n        \n#         self.op_cols = df.columns;  \n#         return df;\n    \n#     def get_feature_names_in(self, X, y=None, **params): \n#         return self.ip_cols;    \n    \n#     def get_feature_names_out(self, X, y=None, **params): \n#         return self.op_cols;\n\n# train_df['outcome'] = train_df['outcome'].map({\"lived\":1,\"died\":2,\"euthanized\":3}).astype(np.int8)\n# original_df['outcome'] = original_df['outcome'].map({\"lived\":1,\"died\":2,\"euthanized\":3}).astype(np.int8)\n\nclass CatEncoder(TransformerMixin, BaseEstimator):\n    def __init__(self): pass;\n    \n    def fit(self, X, y= None):\n#         self.ip_cols = X.columns;\n        return self\n    \n    def transform(self, X, y= None):\n        df = X.copy()\n        df['surgery'] = df['surgery'].map({'yes': 1, 'no': 2}).astype(np.int8)\n        df['age']     = df['age'].map({'adult': 1, 'young': 2}).astype(np.int8)\n        df['temp_of_extremities'] = df['temp_of_extremities'].map({'None': 0, \"normal\":1, \"warm\": 2, \"cool\":3, \"cold\":4}).astype(np.int8)\n        df['peripheral_pulse'] = df['peripheral_pulse'].map({\"None\": 0, \"normal\":1, \"increased\": 2, \"reduced\":3, \"absent\":4}).astype(np.int8)\n        df['mucous_membrane'] = df['mucous_membrane'].map({\"None\": 0,\"normal_pink\":1,\"bright_pink\":2, \"pale_pink\":3 , \"pale_cyanotic\": 4, \"bright_red\":5, \"injected\": 5, \"dark_cyanotic\": 6}).astype(np.int8);\n        ## \"3\": 2, \"more_3_sec\": 2\n        df['capillary_refill_time'] = df['capillary_refill_time'].map({\"None\": 0, \"less_3_sec\":1, \"3\": 2, \"more_3_sec\": 2}).astype(np.int8)\n        ##'slight': 0, \"moderate\": 0需要改动\n        df['pain'] = df['pain'].map({\"None\": 0, \"alert\" : 1, \"no_pain\": 1, \"depressed\": 2, \"mild_pain\": 3, 'slight': 0, \"moderate\": 0, \"severe_pain\": 4, \"extreme_pain\": 5}).astype(np.int8)\n        ## 'distend_small':0\n        df['peristalsis'] =df['peristalsis'].map({\"None\": 0, \"hypermotile\": 1, 'distend_small':0, \"normal\": 2,\"hypomotile\": 3, \"absent\": 4}).astype(np.int8)\n        df['abdominal_distention'] = df['abdominal_distention'].map({\"None\": 0, \"none\": 1, \"slight\": 2, \"moderate\": 3, \"severe\": 4}).astype(np.int8)\n        df['nasogastric_tube'] = df['nasogastric_tube'].map({\"None\": 0, \"none\": 1, \"slight\": 2, \"significant\": 3}).astype(np.int8)\n        ## 'slight':0\n        df['nasogastric_reflux'] = df['nasogastric_reflux'].map({\"None\": 0, \"none\": 1, 'slight':0, \"less_1_liter\": 2, \"more_1_liter\": 3}).astype(np.int8)\n        ##  'serosanguious':0\n        df['rectal_exam_feces'] = df['rectal_exam_feces'].map({\"None\": 0, \"normal\": 1, \"increased\": 2, \"decreased\": 3, \"absent\": 4, 'serosanguious':0}).astype(np.int8)\n        df['abdomen'] = df['abdomen'].map({\"None\":0, \"normal\": 1, \"other\": 2, \"firm\": 3, \"distend_small\": 4, \"distend_large\": 5}).astype(np.int8)\n        df['abdomo_appearance'] = df['abdomo_appearance'].map({\"None\":0, \"clear\": 1, \"cloudy\": 2, \"serosanguious\": 3}).astype(np.int8)\n        df['surgical_lesion'] =df['surgical_lesion'].map({\"yes\": 1, \"no\": 0}).astype(np.int8)\n        df['cp_data'] = df['cp_data'].map({\"yes\": 1, \"no\": 0}).astype(np.int8)\n        \n#         #  Encoding the lesion 2 as 0/ non-zero:-\n#         df['lesion_2'] = df['lesion_2'].clip(0, 1).astype(np.int8)\n        \n        df[['hospital_number', 'lesion_1','lesion_2','lesion_3']] = df[['hospital_number', 'lesion_1','lesion_2','lesion_3']].astype(int)\n#         self.op_cols = df.columns; \n        return df;\n    \n#     def get_feature_names_in(self, X, y=None): \n#         return self.ip_cols;    \n    \n#     def get_feature_names_out(self, X, y=None): \n#         return self.op_cols;\n\n\n\nsteps1 = [\n         (\"Imputer\", ColumnTransformer([\n                                         (\"num_\",\n                                          Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler()),]),\n                                          ['rectal_temp','pulse','respiratory_rate','nasogastric_reflux_ph','packed_cell_volume','total_protein','abdomo_protein']),\n                                        ],\n                                          remainder = SimpleImputer(strategy = \"most_frequent\"),verbose_feature_names_out = True\n                                       )),\n#          ('CatEncoder', CatEncoder()),\n        ]\npipeline1 = Pipeline(steps = steps1, verbose = False)\ndisplay(pipeline1)\n\nXtrain, ytrain = train_df.drop(target, axis=1, errors = 'ignore'), train_df[target]\ntrain_prepared1 = pipeline1.fit_transform(Xtrain, ytrain)\ntest_prepared1 = pipeline1.transform(test_df)\ndisplay(train_prepared1)\ndisplay(test_prepared1)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.729961Z","iopub.execute_input":"2023-09-21T00:51:53.730310Z","iopub.status.idle":"2023-09-21T00:51:53.829551Z","shell.execute_reply.started":"2023-09-21T00:51:53.730281Z","shell.execute_reply":"2023-09-21T00:51:53.828227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipline后分析","metadata":{}},{"cell_type":"code","source":"## 信息增益，互信息\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_classif(X, y)\n    columns = full_pipeline.get_feature_names_out()\n    columns = np.delete(columns, len(columns) - 1)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nX = train_prepared[:, :-1].copy()\ny = train_prepared[:, -1]\n\nmi_scores = make_mi_scores(X, y)\ndisplay(mi_scores[::3]) # show a few features with their MI scores\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:53.831345Z","iopub.execute_input":"2023-09-21T00:51:53.831975Z","iopub.status.idle":"2023-09-21T00:51:54.544275Z","shell.execute_reply.started":"2023-09-21T00:51:53.831944Z","shell.execute_reply":"2023-09-21T00:51:54.542538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data=train_df, x='hospital_number', hue='outcome', fill=True)\nplt.title(\"Distribution of hospital_number, by outcome\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:54.546455Z","iopub.execute_input":"2023-09-21T00:51:54.547169Z","iopub.status.idle":"2023-09-21T00:51:54.978078Z","shell.execute_reply.started":"2023-09-21T00:51:54.547134Z","shell.execute_reply":"2023-09-21T00:51:54.976573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA分析","metadata":{}},{"cell_type":"code","source":"train_X = train_prepared[:, :-1]\ntrain_y = train_prepared[:, -1]\npca = PCA(n_components = 10)\nX2D = pca.fit_transform(train_X)\ndisplay(pd.DataFrame(X2D))\ndisplay(pd.DataFrame(pca.explained_variance_ratio_))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:54.979524Z","iopub.execute_input":"2023-09-21T00:51:54.979884Z","iopub.status.idle":"2023-09-21T00:51:55.095736Z","shell.execute_reply.started":"2023-09-21T00:51:54.979846Z","shell.execute_reply":"2023-09-21T00:51:55.094293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 网格搜索相关","metadata":{}},{"cell_type":"code","source":"# # Pipeline specifics:-\n# from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler;\n# from sklearn.impute import SimpleImputer as SI;\n# from sklearn.model_selection import (RepeatedStratifiedKFold as RSKF, \n#                                      StratifiedKFold as SKF,\n#                                      KFold, \n#                                      RepeatedKFold as RKF, \n#                                      cross_val_score);\n# from sklearn.inspection import permutation_importance;\n# from sklearn.feature_selection import mutual_info_classif, RFE;\n# from sklearn.pipeline import Pipeline, make_pipeline;\n# from sklearn.base import BaseEstimator, TransformerMixin;\n# from sklearn.compose import ColumnTransformer;\n\n# # ML Model training:-\n# from sklearn.metrics import f1_score, confusion_matrix, make_scorer;\n# from xgboost import DMatrix, XGBClassifier;\n# from lightgbm import LGBMClassifier, log_evaluation, early_stopping;\n# from catboost import CatBoostClassifier, Pool;\n# from sklearn.ensemble import (RandomForestClassifier as RFC, \n#                               ExtraTreesClassifier as ETC,\n#                               AdaBoostClassifier as ABC,\n#                               BaggingClassifier as BC,\n#                               HistGradientBoostingClassifier as HGBC\n#                              );\n# from sklearn.linear_model import LogisticRegression as LC;\n# Mdl_Master = \\\n# {'CBC': CatBoostClassifier(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n#                               'objective'           : \"MultiClass\",\n#                               'eval_metric'         : \"Accuracy\",\n#                               'classes_count'       : 3,\n#                               'bagging_temperature' : 0.10,\n#                               'colsample_bylevel'   : 0.75,\n#                               'iterations'          : 1000,\n#                               'learning_rate'       : 0.075,\n#                               'od_wait'             : 3,\n#                               'max_depth'           : 4,\n#                               'l2_leaf_reg'         : 0.85,\n#                               'min_data_in_leaf'    : 6,\n#                               'random_strength'     : 0.65, \n#                               'max_bin'             : 80,\n#                               'verbose'             : 0,\n#                               'use_best_model'      : True,\n#                            }\n#                          ), \n\n#   'LGBMC': LGBMClassifier(**{'device'            : \"gpu\" if CFG.gpu_switch == \"ON\" else \"cpu\",\n#                              'objective'         : 'multiclass',\n#                              'metric'            : 'none',\n#                              'boosting_type'     : 'gbdt',\n#                              'random_state'      : CFG.state,\n#                              'colsample_bytree'  : 0.5,\n#                              'subsample'         : 0.65,\n#                              'learning_rate'     : 0.08,\n#                              'max_depth'         : 4,\n#                              'n_estimators'      : 1000,\n#                              'num_leaves'        : 72,                    \n#                              'reg_alpha'         : 0.01,\n#                              'reg_lambda'        : 1.75,\n#                              'verbose'           : -1,\n#                          }\n#                       ),\n\n#   'XGBC': XGBClassifier(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n#                            'objective'          : 'multi:softprob',\n#                            'random_state'       : CFG.state,\n#                            'colsample_bytree'   : 0.7,\n#                            'learning_rate'      : 0.07,\n#                            'max_depth'          : 4,\n#                            'n_estimators'       : 1100,                         \n#                            'reg_alpha'          : 0.025,\n#                            'reg_lambda'         : 1.75,\n#                            'min_child_weight'   : 5,\n#                            'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n#                         }\n#                        ),\n \n#    'RFC' : RFC(n_estimators     = 150, \n#                criterion        = 'gini',\n#                max_depth        = 4,\n#                min_samples_leaf = 5,\n#                max_features     = 'log2',\n#                bootstrap        = True,\n#                oob_score        = True,\n#                random_state     = CFG.state,\n#                verbose          =0,\n#               ), \n \n#   \"HGBC\" : HGBC(loss              = 'categorical_crossentropy',\n#                 learning_rate     = 0.075,\n#                 early_stopping    = True,\n#                 max_iter          = 200,\n#                 max_depth         = 4,\n#                 min_samples_leaf  = 5,\n#                 l2_regularization = 1.75,\n#                 scoring           = myscorer,\n#                 random_state      = CFG.state,\n#                )\n# };","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:55.097772Z","iopub.execute_input":"2023-09-21T00:51:55.098297Z","iopub.status.idle":"2023-09-21T00:51:55.120827Z","shell.execute_reply.started":"2023-09-21T00:51:55.098250Z","shell.execute_reply":"2023-09-21T00:51:55.119282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 单模型----> 随机森林、LightGBM、XGBoost、CatBoost","metadata":{}},{"cell_type":"markdown","source":"## 均值融合、加权融合、Stacking融合(分类问题用逻辑回归、否则用贝叶斯回归(bayesianRidge))","metadata":{}},{"cell_type":"code","source":"## trick","metadata":{"execution":{"iopub.status.busy":"2023-09-21T00:51:55.123939Z","iopub.execute_input":"2023-09-21T00:51:55.124774Z","iopub.status.idle":"2023-09-21T00:51:55.134068Z","shell.execute_reply.started":"2023-09-21T00:51:55.124721Z","shell.execute_reply":"2023-09-21T00:51:55.132619Z"},"trusted":true},"execution_count":null,"outputs":[]}]}