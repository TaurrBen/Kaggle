{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T09:19:02.121430Z","iopub.execute_input":"2023-09-21T09:19:02.121823Z","iopub.status.idle":"2023-09-21T09:19:02.148937Z","shell.execute_reply.started":"2023-09-21T09:19:02.121796Z","shell.execute_reply":"2023-09-21T09:19:02.147496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 导入必要的库","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom colorama import Fore, Style, init\n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config\nset_config(transform_output = \"pandas\")\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 50)\nrandom_state = 202309\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold,KFold, RepeatedKFold, cross_val_score\n\n# ML Model training:-\nfrom sklearn.metrics import f1_score, confusion_matrix, make_scorer\nfrom xgboost import DMatrix, XGBClassifier\nfrom lightgbm import LGBMClassifier, log_evaluation, early_stopping\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,BaggingClassifier,HistGradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression;","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:43.637383Z","iopub.execute_input":"2023-09-21T09:20:43.637825Z","iopub.status.idle":"2023-09-21T09:20:43.655505Z","shell.execute_reply.started":"2023-09-21T09:20:43.637796Z","shell.execute_reply":"2023-09-21T09:20:43.653999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 读取数据","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e22/train.csv', index_col = 'id')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e22/test.csv', index_col = 'id')\noriginal_df = pd.read_csv('/kaggle/input/horse-survival-dataset/horse.csv')\nsubmission_df = pd.read_csv('/kaggle/input/playground-series-s3e22/sample_submission.csv', index_col = 'id')\ntrain_df.index.name = None\ntest_df.index.name = None\n\noriginal_df.index = range(len(original_df))\noriginal_df.index += max(test_df.index) + 1\noriginal_df = original_df.reindex(train_df.columns, axis=1)\n\n# 融合的数据\ntrainANDoriginal_df = pd.concat([train_df, original_df], axis=0, ignore_index = True)\n\nfeature = train_df.columns[:-1]\ntarget = train_df.columns[-1]\ntarget_map = {\"lived\":1,\"died\":2,\"euthanized\":3}","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:22:01.347001Z","iopub.execute_input":"2023-09-21T09:22:01.347390Z","iopub.status.idle":"2023-09-21T09:22:01.399604Z","shell.execute_reply.started":"2023-09-21T09:22:01.347362Z","shell.execute_reply":"2023-09-21T09:22:01.398352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## datadict","metadata":{}},{"cell_type":"code","source":"# with open('/kaggle/input/horse-colic/datadict.txt', 'r') as file:\n#    for line in file:\n#        print(line)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.238156Z","iopub.execute_input":"2023-09-21T09:19:02.238570Z","iopub.status.idle":"2023-09-21T09:19:02.244466Z","shell.execute_reply.started":"2023-09-21T09:19:02.238540Z","shell.execute_reply":"2023-09-21T09:19:02.243178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 常用函数","metadata":{}},{"cell_type":"code","source":"def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    print(style + color + text + style)\n\n# 字典编码函数  排序映射赋值（有序离散object处理）\ndef change_object_cols(se):\n    value = se.unique().tolist()\n    value.sort()\n    return se.map(pd.Series(range(len(value),index=value))).values\n# 多变量分析  将两个变量融合进而分析\ndef combine_feature(df):\n    cols = df.columns\n    feature1 = df[cols[0]].astype(str).values.tolist()\n    feature2 = df[cols[1]].astype(str).values.tolist()\n    return pd.Series([feature1[i] + '&' + feature2[i] for i in range(df.shape[0])])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.247809Z","iopub.execute_input":"2023-09-21T09:19:02.248183Z","iopub.status.idle":"2023-09-21T09:19:02.261847Z","shell.execute_reply.started":"2023-09-21T09:19:02.248154Z","shell.execute_reply":"2023-09-21T09:19:02.260690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 对抗验证 adversarial CV","metadata":{}},{"cell_type":"code","source":"# # Performing adversarial CV between the 2 specified datasets:-\n# def Do_AdvCV(df1:pd.DataFrame, df2:pd.DataFrame, source1:str, source2:str):\n#     \"This function performs an adversarial CV between the 2 provided datasets if needed by the user\";\n    \n#     # Adversarial CV per column:-\n#     ftre = pp.test.select_dtypes(include = np.number).\\\n#     drop(columns = ['id', \"Source\"], errors = 'ignore').columns;\n#     adv_cv = {};\n\n#     for col in ftre:\n#         shuffle_state = np.random.randint(low = 10, high = 100, size= 1);\n\n#         full_df = \\\n#         pd.concat([df1[[col]].assign(Source = source1), df2[[col]].assign(Source = source2)], \n#                   axis=0, ignore_index = True).\\\n#         sample(frac = 1.00, random_state = shuffle_state);\n\n#         full_df = full_df.assign(Source_Nb = full_df['Source'].eq(source2).astype(np.int8));\n\n#         # Checking for adversarial CV:-\n#         model = LGBMClassifier(random_state = CFG.state, max_depth = 6, learning_rate = 0.05);\n#         cv    = all_cv['SKF'];\n#         score = np.mean(cross_val_score(model, \n#                                         full_df[[col]], \n#                                         full_df.Source_Nb, \n#                                         scoring= 'roc_auc', \n#                                         cv     = cv)\n#                        );\n#         adv_cv.update({col: round(score, 4)});\n#         collect();\n    \n#     del ftre;\n#     collect();\n    \n#     fig, ax = plt.subplots(1,1,figsize = (12, 5));\n#     pd.Series(adv_cv).plot.bar(color = 'tab:blue', ax = ax);\n#     ax.axhline(y = 0.60, color = 'red', linewidth = 2.75);\n#     ax.grid(**CFG.grid_specs); \n#     plt.yticks(np.arange(0.0, 0.81, 0.05));\n#     plt.show();\n    \n# # Implementing the adversarial CV:-\n# if CFG.adv_cv_req == \"Y\":\n#     PrintColor(f\"\\n---------- Adversarial CV - Train vs Original ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.train, df2 = pp.original, source1 = 'Train', source2 = 'Original');\n    \n#     PrintColor(f\"\\n---------- Adversarial CV - Train vs Test ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.train, df2 = pp.test, source1 = 'Train', source2 = 'Test');\n    \n#     PrintColor(f\"\\n---------- Adversarial CV - Original vs Test ----------\\n\", \n#                color = Fore.MAGENTA);\n#     Do_AdvCV(df1 = pp.original, df2 = pp.test, source1 = 'Original', source2 = 'Test');   \n    \n# if CFG.adv_cv_req == \"N\":\n#     PrintColor(f\"\\nAdversarial CV is not needed\\n\", color = Fore.RED);\n    \n# collect();\n# print();","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.263842Z","iopub.execute_input":"2023-09-21T09:19:02.264302Z","iopub.status.idle":"2023-09-21T09:19:02.276248Z","shell.execute_reply.started":"2023-09-21T09:19:02.264265Z","shell.execute_reply":"2023-09-21T09:19:02.274827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Train\nPrintColor(f\"\\nTrain set head\", color = Fore.BLUE)\ndisplay(train_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTrain set description\", color = Fore.BLUE)\ndisplay(train_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTrain set Information\", color = Fore.BLUE)\ndisplay(train_df.info())\n\n# Original\nPrintColor(f\"\\nOriginal set head\", color = Fore.BLUE)\ndisplay(original_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nOriginal set description\", color = Fore.BLUE)\ndisplay(original_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nOriginal set Information\", color = Fore.BLUE)\ndisplay(original_df.info())\n\n# TrainANDoriginal\nPrintColor(f\"\\nTrainANDoriginal set head\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTrainANDoriginal set description\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTrainANDoriginal set Information\", color = Fore.BLUE)\ndisplay(trainANDoriginal_df.info())\n\n# Test\nPrintColor(f\"\\nTest set head\", color = Fore.BLUE)\ndisplay(test_df.head(5).style.format(precision = 3))\nPrintColor(f\"\\nTest set description\", color = Fore.BLUE)\ndisplay(test_df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).transpose().\n                            drop(columns = ['count'], errors = 'ignore').\n                            style.format(formatter = '{:,.2f}').\n                            background_gradient(cmap = 'Purples'))\nPrintColor(f\"\\nTest set Information\", color = Fore.BLUE)\ndisplay(test_df.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.278505Z","iopub.execute_input":"2023-09-21T09:19:02.279472Z","iopub.status.idle":"2023-09-21T09:19:02.770770Z","shell.execute_reply.started":"2023-09-21T09:19:02.279406Z","shell.execute_reply":"2023-09-21T09:19:02.769249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.hospital_number 有重复、出现数值很大的编号、是否应该当成离散量重新编一下码----作为类别变量----独热编码\n##   hospital_number 在test中出现但在train中没出现 ---- 需要test和train合并编码\n## 2.lesion_1、lesion_2、lesion_3数据比较有问题，需要如何处理\n##  2.1lesion_2为非0，outcome为非died\n##     可处理df['lesion_2'] = df['lesion_2'].apply(lambda x:1 if x>0 else 0)  （预测died变好？）\n##     作为类别变量处理\n##  2.2lesion_3只有两个值，lession_3非0，outcome非died\n##     同上处理？\n##     作为类别变量处理\n##  2.3查看到其特征解释貌似可以拆解开分为多个特征\n##     见datadict","metadata":{}},{"cell_type":"markdown","source":"## other tricks\n## 1.马儿的温度影响outcome，偏离37.8越多越容易死\n##   df[\"deviation_from_normal_temp\"] = df[\"rectal_temp\"].apply(lambda x: abs(x - 37.8))\n## 2.pulse, respiatory_rate, packed_cell_volume是否可以如1","metadata":{}},{"cell_type":"code","source":"# 数据正确性验证\n# 缺失值\n# 异常值\n# 规律性分析 （单变量分析、多变量分析）\n# 离散型变量区分、名义型变量（男女-10）、有序（考虑是否当成连续）","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.772894Z","iopub.execute_input":"2023-09-21T09:19:02.773350Z","iopub.status.idle":"2023-09-21T09:19:02.778831Z","shell.execute_reply.started":"2023-09-21T09:19:02.773311Z","shell.execute_reply":"2023-09-21T09:19:02.777592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 缺失数据分析1\nPrintColor(f\"\\nTrain set null values\", color = Fore.BLUE)\ntrain_missing = train_df.isnull().sum() * 100 / len(train_df)\ntrain_missing = train_missing[train_missing != 0]\ntrain_missing = pd.DataFrame({'missing percent': train_missing})\ntrain_missing = train_missing.sort_values('missing percent', ascending=False)\ndisplay(train_missing.transpose().style.format(precision = 3))\nfor col in train_missing.index:\n    print(col,train_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nOriginal set null values\", color = Fore.BLUE)\noriginal_missing = original_df.isnull().sum() * 100 / len(original_df)\noriginal_missing = original_missing[original_missing != 0]\noriginal_missing = pd.DataFrame({'missing percent': original_missing})\noriginal_missing = original_missing.sort_values('missing percent', ascending=False)\ndisplay(original_missing.transpose().style.format(precision = 3))\nfor col in original_missing.index:\n    print(col,original_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nTrainAndOriginal set null values\", color = Fore.BLUE)\ntrainANDoriginal_missing = trainANDoriginal_df.isnull().sum() * 100 / len(trainANDoriginal_df)\ntrainANDoriginal_missing = trainANDoriginal_missing[trainANDoriginal_missing != 0]\ntrainANDoriginal_missing = pd.DataFrame({'missing percent': trainANDoriginal_missing})\ntrainANDoriginal_missing = trainANDoriginal_missing.sort_values('missing percent', ascending=False)\ndisplay(trainANDoriginal_missing.transpose().style.format(precision = 3))\nfor col in trainANDoriginal_missing.index:\n    print(col,trainANDoriginal_df[col].unique())#此处缺失值如何去补充\n    \nPrintColor(f\"\\nTest set null values\", color = Fore.BLUE)\ntest_missing = test_df.isnull().sum() * 100 / len(test_df)\ntest_missing = test_missing[test_missing != 0]\ntest_missing = pd.DataFrame({'missing percent': test_missing})\ntest_missing = test_missing.sort_values('missing percent', ascending=False)\ndisplay(test_missing.transpose().style.format(precision = 3))\nfor col in test_missing.index:\n    print(col,test_df[col].unique())#此处缺失值如何去补充","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.780793Z","iopub.execute_input":"2023-09-21T09:19:02.781274Z","iopub.status.idle":"2023-09-21T09:19:02.883649Z","shell.execute_reply.started":"2023-09-21T09:19:02.781237Z","shell.execute_reply":"2023-09-21T09:19:02.882238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 缺失数据分析2\ndef summary_df(train_df,original_df,trainANDoriginal_df,test_df):\n    summary = pd.DataFrame(train_df.dtypes, columns=['dtypes'])\n    summary['train_missing#'] = train_df.isna().sum()\n    summary['train_missing%'] = (train_df.isna().sum())/len(train_df)\n    summary['train_uniques'] = train_df.nunique().values\n    summary['train_count'] = train_df.count().values\n    # summary['train_skew'] = train_df.skew().values\n    summary['original_missing#'] = original_df.isna().sum()\n    summary['original_missing%'] = (original_df.isna().sum())/len(original_df)\n    summary['original_uniques'] = original_df.nunique().values\n    summary['original_count'] = original_df.count().values\n    # summary['original_skew'] = original_df.skew().values\n    summary['test_missing#'] = test_df.isna().sum()\n    summary['test_missing%'] = (test_df.isna().sum())/len(test_df)\n    summary['test_uniques'] = test_df.nunique().values\n    summary['test_count'] = test_df.count().values\n    # summary['test_skew'] = train_df.skew().values\n    summary['trainANDoriginal_missing#'] = trainANDoriginal_df.isna().sum()\n    summary['trainANDoriginal_missing%'] = (trainANDoriginal_df.isna().sum())/len(trainANDoriginal_df)\n    summary['trainANDoriginal_uniques'] = trainANDoriginal_df.nunique().values\n    summary['trainANDoriginal_count'] = trainANDoriginal_df.count().values\n    # summary['trainANDoriginal_skew'] = trainANDoriginal_df.skew().values\n    return summary\nsummary_df(train_df[feature],original_df[feature],trainANDoriginal_df[feature],test_df[feature]).style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:02.885832Z","iopub.execute_input":"2023-09-21T09:19:02.886358Z","iopub.status.idle":"2023-09-21T09:19:03.036952Z","shell.execute_reply.started":"2023-09-21T09:19:02.886315Z","shell.execute_reply":"2023-09-21T09:19:03.035296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.缺失值数据均为object类型\n## 2.缺失值数据中的nan，有些还有none的存在，应该如何处理nan。  检查了一下数据发现是大写的None和小写的none的问题？\n##   好像两者是不同的\n## 3.","metadata":{}},{"cell_type":"code","source":"# 寻找object类型、或者直接规定\ndtypes = train_df.dtypes.to_dict()\ncat_cols = []\nnum_cols = []\nfor column, typ in dtypes.items():\n    if typ == \"object\":\n        cat_cols.append(column)\n    else:\n        num_cols.append(column)\n\n# 独特性验证(类别类、独特类（id等）)\nfor col in cat_cols:\n    print('Train   ',col,train_df[col].unique())\n    print('Original   ',col,original_df[col].unique())\n    if col==target:\n        continue\n    print('Test   ',col,test_df[col].unique())\n\ndef unique_validation(train_df,test_df,original_df,trainANDoriginal_df,features):\n    for feature in features:\n        print('train',feature,train_df[feature].unique())\n        print('test',feature,test_df[feature].unique())\n        print('original',feature,original_df[feature].unique())\n        print('trainANDoriginal',feature,trainANDoriginal_df[feature].unique())\n\n        print('original_unique_to_train',feature,[original_unique_to_train for original_unique_to_train in original_df[feature].unique() if original_unique_to_train not in train_df[feature].unique()])\n        print('test_unique_to_train',feature,[test_unique_to_train for test_unique_to_train in test_df[feature].unique() if test_unique_to_train not in train_df[feature].unique()])\n        print('train_unique_to_original',feature,[train_unique_to_original for train_unique_to_original in train_df[feature].unique() if train_unique_to_original not in original_df[feature].unique()])\n        print('train_unique_to_test',feature,[train_unique_to_test for train_unique_to_test in train_df[feature].unique() if train_unique_to_test not in test_df[feature].unique()])\n        print('original_unique_to_test',feature,[original_unique_to_test for original_unique_to_test in original_df[feature].unique() if original_unique_to_test not in test_df[feature].unique()])\n        print('test_unique_to_original',feature,[test_unique_to_original for test_unique_to_original in test_df[feature].unique() if test_unique_to_original not in original_df[feature].unique()])\n\n        print('trainANDoriginal_unique_to_test',feature,[trainANDoriginal_unique_to_test for trainANDoriginal_unique_to_test in trainANDoriginal_df[feature].unique() if trainANDoriginal_unique_to_test not in test_df[feature].unique()])\n        print('test_unique_to_trainANDoriginal',feature,[test_unique_to_trainANDoriginal for test_unique_to_trainANDoriginal in test_df[feature].unique() if test_unique_to_trainANDoriginal not in trainANDoriginal_df[feature].unique()])\n\nunique_validation(train_df,test_df,original_df,trainANDoriginal_df,['lesion_1','lesion_2','lesion_3'])#,'hospital_number'","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:03.039452Z","iopub.execute_input":"2023-09-21T09:19:03.039892Z","iopub.status.idle":"2023-09-21T09:19:03.118157Z","shell.execute_reply.started":"2023-09-21T09:19:03.039858Z","shell.execute_reply":"2023-09-21T09:19:03.116469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#单变量分布1\nfig, axes = plt.subplots(len(cat_cols), 4, figsize = (20, len(cat_cols)* 2), \n                             gridspec_kw = {'wspace': 0.35, 'hspace': 0.5})\nfor i, column in enumerate(cat_cols):\n    ax1 = axes[i, 0]\n    ax2 = axes[i, 1]\n    ax3 = axes[i, 2]\n    ax4 = axes[i, 3]\n    a = train_df[column].value_counts(normalize = True)\n    a.sort_index().plot.barh(ax = ax1, color = 'purple')\n    ax1.set_title(f\"{column}_Train\")\n    ax1.set(ylabel = '')\n    b = original_df[column].value_counts(normalize = True)\n    b.sort_index().plot.barh(ax = ax2, color = 'purple')\n    ax2.set_title(f\"{column}_Original\")\n    ax2.set(ylabel = '')\n    d = trainANDoriginal_df[column].value_counts(normalize = True)\n    d.sort_index().plot.barh(ax = ax4, color = 'purple')\n    ax4.set_title(f\"{column}_TrainAndOriginal\")\n    ax4.set(ylabel = '')\n    if column==target:\n        continue\n    c = test_df[column].value_counts(normalize = True)\n    c.sort_index().plot.barh(ax = ax3, color = 'purple')\n    ax3.set_title(f\"{column}_Test\")\n    ax3.set(ylabel = '')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:03.120177Z","iopub.execute_input":"2023-09-21T09:19:03.120635Z","iopub.status.idle":"2023-09-21T09:19:16.316018Z","shell.execute_reply.started":"2023-09-21T09:19:03.120594Z","shell.execute_reply":"2023-09-21T09:19:16.314373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.pain  训练集slight  测试集moderate\n##   考虑两者合并之后编码\n## 2.peristalsis 训练集多一个distend_small（1个样本）\n## 3.nasogastric_reflux 训练集多一个slight（1个样本）\n## 4.rectal_exam_feces 训练集多一个serosanguious（1个样本）\n##   234不删除可能也不影响，删除好像也不影响\n## 5.mucous 中的pale_cyanotic分布不太一样（异常突出）\n## 6.abdominal_distention、nasogastric_tube、nasogastric_reflux中的none   \n##   考虑将None和none 都做none（普通字符串none的意义）处理？\n##  （用isnull统计的None被视为无值，但是none被视为普通字符串）\n##   此处处理完之后还需要做一下训练集和测试集分布分析\n","metadata":{}},{"cell_type":"code","source":"# 单变量分布\nfig, axes = plt.subplots(len(num_cols), 5, figsize = (20, len(num_cols)* 4), \n                             gridspec_kw = {'wspace': 0.35, 'hspace': 0.5, 'width_ratios': [0.80,0.20, 0.20, 0.20, 0.20]})\nfor i, column in enumerate(num_cols):\n    ax = axes[i,0]\n    a = train_df[column]\n    a.plot(ax = ax,kind='density', color = 'purple',label='Train')\n    b = original_df[column]\n    b.plot(ax = ax,kind='density', color = 'pink',label='Original')\n    if column not in ['lesion_3','outcome']:\n        c = test_df[column]\n        c.plot(ax = ax,kind='density', color = 'red',label='Test')\n    d = trainANDoriginal_df[column]\n    d.plot(ax = ax,kind='density', color = 'green',label='TrAndOr')\n    ax.set_title(f\"{column}_Train_vs_Test_vs_Original_vs_TrAndOr\")\n    ax.set(ylabel = '')\n    ax.legend(['Train','Original','Test','TrAndOr'])\n     \n    \n    ax = axes[i,1]\n    sns.boxplot(y = a, width = 0.25,saturation = 0.90, linewidth = 0.90, fliersize= 2.25, color = '#037d97',ax = ax)\n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"Train\",fontsize = 9, fontweight= 'bold')\n    ax = axes[i,2]\n    sns.boxplot(y = b, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#E4591E',ax = ax) \n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"Original\",fontsize = 9, fontweight= 'bold')\n    ax = axes[i,4]\n    sns.boxplot(y = d, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#B78C25',ax = ax) \n    ax.set(xlabel = '', ylabel = '')\n    ax.set_title(f\"TrainAndOriginal\",fontsize = 9, fontweight= 'bold')\n    if column not in ['lesion_3','outcome']:\n        ax = axes[i,3]\n        sns.boxplot(y = c, width = 0.25, fliersize= 2.25,saturation = 0.6, linewidth = 0.90, color = '#544E91',ax = ax)\n        ax.set(xlabel = '', ylabel = '')\n        ax.set_title(f\"Test\",fontsize = 9, fontweight= 'bold')\n# 多变量分析","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:16.318743Z","iopub.execute_input":"2023-09-21T09:19:16.319206Z","iopub.status.idle":"2023-09-21T09:19:30.977864Z","shell.execute_reply.started":"2023-09-21T09:19:16.319171Z","shell.execute_reply":"2023-09-21T09:19:30.976739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.单变量分析，训练集和测试集分布基本一致，除了hospital_number和lesion_3差别\n## 2.hospital_number\n## 3.lesion_3","metadata":{}},{"cell_type":"code","source":"# 联合outcome的单变量分布\ndef plot_count(train_df,original_df,trainANDoriginal_df,columns,hue,other_features=['lesion_1','lesion_2','lesion_3','hospital_number']):\n    columns = columns + other_features\n    n_rows = len(columns)\n    fig, axes = plt.subplots(n_rows, 3, figsize=(17, 4 * n_rows))\n    for i, column in enumerate(columns):\n        ax = axes[i,0]\n        sns.countplot(data=train_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'Tarin_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,1]\n        sns.countplot(data=original_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'Original_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,2]\n        sns.countplot(data=trainANDoriginal_df, x=column, ax=ax,hue=hue)\n        ax.set_title(f'TarinAndOriginal_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n\n    ylim_top = ax.get_ylim()[1]\n    ax.set_ylim(top=ylim_top * 1.1)\n    for i in range(len(columns), len(axes)):\n        axes[i,0].axis('off')\n        axes[i,1].axis('off')\n        axes[i,2].axis('off')\n\n    # fig.suptitle(plotname, fontsize=25, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \nplot_count(train_df,original_df,trainANDoriginal_df,[cat_col for cat_col in cat_cols if cat_col != target],target,other_features=['lesion_2','lesion_3'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:30.983108Z","iopub.execute_input":"2023-09-21T09:19:30.983542Z","iopub.status.idle":"2023-09-21T09:19:50.204717Z","shell.execute_reply.started":"2023-09-21T09:19:30.983508Z","shell.execute_reply":"2023-09-21T09:19:50.202891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 联合outcome的单变量分布\ndef plot_kde(train_df,original_df,trainANDoriginal_df,columns,hue):\n    columns = columns\n    n_rows = len(columns)\n    fig, axes = plt.subplots(n_rows, 3, figsize=(17, 4 * n_rows))\n    for i, column in enumerate(columns):\n        ax = axes[i,0]\n        sns.kdeplot(data=train_df, x=column, ax=ax,hue=hue,fill=True)\n        ax.set_title(f'Tarin_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,1]\n        sns.kdeplot(data=original_df, x=column, ax=ax,hue=hue,fill=True)\n        ax.set_title(f'Original_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n            \n        ax = axes[i,2]\n        sns.kdeplot(data=trainANDoriginal_df, x=column, ax=ax,hue=hue,fill=True)\n        ax.set_title(f'TarinAndOriginal_{column}_Counts', fontsize=18)\n        ax.set_xlabel(None, fontsize=16)\n        ax.set_ylabel(None, fontsize=16)\n        ax.tick_params(axis='x', rotation=10)\n        for p in ax.patches:\n            value = int(p.get_height())\n            ax.annotate(f'{value:.0f}', (p.get_x() + p.get_width() / 2, p.get_height()),\n                           ha='center', va='bottom', fontsize=9)\n\n    ylim_top = ax.get_ylim()[1]\n    ax.set_ylim(top=ylim_top * 1.1)\n    for i in range(len(columns), len(axes)):\n        axes[i,0].axis('off')\n        axes[i,1].axis('off')\n        axes[i,2].axis('off')\n\n    # fig.suptitle(plotname, fontsize=25, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \nplot_kde(train_df,original_df,trainANDoriginal_df,num_cols,target)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:19:50.207229Z","iopub.execute_input":"2023-09-21T09:19:50.207906Z","iopub.status.idle":"2023-09-21T09:20:03.894630Z","shell.execute_reply.started":"2023-09-21T09:19:50.207873Z","shell.execute_reply":"2023-09-21T09:20:03.890900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 双变量配对分析\ndef plot_pair(train_df,num_var,target,plotname):\n    g = sns.pairplot(data=train_df, x_vars=num_var, y_vars=num_var, hue=target, corner=True)\n    g._legend.set_bbox_to_anchor((0.8, 0.7))\n    g._legend.set_title(target)\n    g._legend.loc = 'upper center'\n    g._legend.get_title().set_fontsize(14)\n    for item in g._legend.get_texts():\n        item.set_fontsize(14)\n\n    plt.suptitle(plotname, ha='center', fontweight='bold', fontsize=25, y=0.98)\n    plt.show()\n\n# plot_pair(train_df,num_cols,target,plotname = 'Scatter Matrix with Target Of Train')\n# plt.tight_layout()\n# plot_pair(original_df,num_cols,target,plotname = 'Scatter Matrix with Target Of Original')\n# plt.tight_layout()\n# plot_pair(trainANDoriginal_df,num_cols,target,plotname = 'Scatter Matrix with Target Of TrainAndOriginal')\n# plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:03.896376Z","iopub.execute_input":"2023-09-21T09:20:03.896756Z","iopub.status.idle":"2023-09-21T09:20:03.907162Z","shell.execute_reply.started":"2023-09-21T09:20:03.896718Z","shell.execute_reply":"2023-09-21T09:20:03.905835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"method = \"pearson\"\nplt.figure(figsize=(15,15))\ncorr = train_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_train\")\n\nplt.figure(figsize=(15,15))\ncorr = original_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_original\")\n\nplt.figure(figsize=(15,15))\ncorr = test_df[[num_col for num_col in num_cols if num_col != target]].corr(method = \"pearson\")# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_test\")\n\nplt.figure(figsize=(15,15))\ncorr = trainANDoriginal_df[num_cols].corr(method = method)# 皮尔逊相关\nmask = np.triu(np.triu(corr))\nsns.heatmap(data = corr,annot=True, cmap = 'Blues', mask=mask)\nplt.title(method+\"_corr_of_trainANDoriginal\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:03.909137Z","iopub.execute_input":"2023-09-21T09:20:03.909529Z","iopub.status.idle":"2023-09-21T09:20:07.242457Z","shell.execute_reply.started":"2023-09-21T09:20:03.909501Z","shell.execute_reply":"2023-09-21T09:20:07.240970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.pulse 和 respiratory_rate、packed_cell_volume正相关比较明显\n## 2.nasogastric_reflux_ph 和total_protein、abdomo_protein分别呈负相关、正相关\n## 3.total_protein和abdomo_protein呈正相关\n## 4.lesion_2和lesion_3呈正相关","metadata":{}},{"cell_type":"markdown","source":"## 卡方检验","metadata":{}},{"cell_type":"code","source":"def chi_squared_test(df, input_var, target_var, significance_level=0.05):\n    contingency_table = pd.crosstab(df[input_var], df[target_var])\n    chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n    \n    if p < significance_level:\n        print(f'\\033[32m{input_var} has a significant relationship with the target variable.\\033[0m') \n    else:\n        print(f'\\033[31m{input_var} does not have a significant relationship with the target variable.\\033[0m')  \n\nfor cat_col in cat_cols:\n    chi_squared_test(train_df, cat_col, target)\n    chi_squared_test(original_df, cat_col, target)\n    chi_squared_test(trainANDoriginal_df, cat_col, target)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:07.244269Z","iopub.execute_input":"2023-09-21T09:20:07.244665Z","iopub.status.idle":"2023-09-21T09:20:07.879029Z","shell.execute_reply.started":"2023-09-21T09:20:07.244634Z","shell.execute_reply":"2023-09-21T09:20:07.877065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **特征工程**","metadata":{}},{"cell_type":"code","source":"## 1.通用组合特征\n## 2.业务统计特征\n## 我们需要融合？\n## NLP特征池衍生（from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer）","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:07.881836Z","iopub.execute_input":"2023-09-21T09:20:07.882475Z","iopub.status.idle":"2023-09-21T09:20:07.890024Z","shell.execute_reply.started":"2023-09-21T09:20:07.882420Z","shell.execute_reply":"2023-09-21T09:20:07.888014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(num_cols,f'\\n',cat_cols)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:07.892307Z","iopub.execute_input":"2023-09-21T09:20:07.892828Z","iopub.status.idle":"2023-09-21T09:20:07.908660Z","shell.execute_reply.started":"2023-09-21T09:20:07.892782Z","shell.execute_reply":"2023-09-21T09:20:07.907473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **pipeline**","metadata":{}},{"cell_type":"code","source":"# num_pipeline = Pipeline([\n#     ('imputer', SimpleImputer(strategy=\"median\")),\n#     ('std_scaler', StandardScaler()),\n# ])\n# cat_pipeline = Pipeline([\n#     ('imputer', SimpleImputer(strategy=\"constant\", fill_value=\"NaN\")),\n#     ('encoder', OrdinalEncoder())\n# ])\n# full_pipeline = ColumnTransformer([\n#     (\"num\", num_pipeline, num_cols),\n#     (\"cat\", cat_pipeline, cat_cols),\n# ])\n# train_prepared = full_pipeline.fit_transform(train_df)\n# display(train_df)\n# display(pd.DataFrame(train_prepared))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:07.910968Z","iopub.execute_input":"2023-09-21T09:20:07.911453Z","iopub.status.idle":"2023-09-21T09:20:07.927069Z","shell.execute_reply.started":"2023-09-21T09:20:07.911412Z","shell.execute_reply":"2023-09-21T09:20:07.925743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CatEncoder(TransformerMixin, BaseEstimator):\n    def __init__(self): pass;\n    \n    def fit(self, X, y= None):\n        self.ip_cols = X.columns;\n        return self\n    \n    def transform(self, X, y= None):\n        df = X.copy()\n        df['surgery'] = df['surgery'].map({'yes': 1, 'no': 2}).astype(np.int8)\n        df['age']     = df['age'].map({'adult': 1, 'young': 2}).astype(np.int8)\n        df['temp_of_extremities'] = df['temp_of_extremities'].map({'None': 0, \"normal\":1, \"warm\": 2, \"cool\":3, \"cold\":4}).astype(np.int8)\n        df['peripheral_pulse'] = df['peripheral_pulse'].map({\"None\": 0, \"normal\":1, \"increased\": 2, \"reduced\":3, \"absent\":4}).astype(np.int8)\n        df['mucous_membrane'] = df['mucous_membrane'].map({\"None\": 0,\"normal_pink\":1,\"bright_pink\":2, \"pale_pink\":3 , \"pale_cyanotic\": 4, \"bright_red\":5, \"injected\": 5, \"dark_cyanotic\": 6}).astype(np.int8);\n        ## \"3\": 2, \"more_3_sec\": 2\n        df['capillary_refill_time'] = df['capillary_refill_time'].map({\"None\": 0, \"less_3_sec\":1, \"3\": 2, \"more_3_sec\": 2}).astype(np.int8)\n        ## 'slight': 0, \"moderate\": 0需要改动\n        df['pain'] = df['pain'].map({\"None\": 0, \"alert\" : 1, \"no_pain\": 1, \"depressed\": 2, \"mild_pain\": 3, 'slight': 0, \"moderate\": 0, \"severe_pain\": 4, \"extreme_pain\": 5}).astype(np.int8)\n        ## 'distend_small':0\n        df['peristalsis'] =df['peristalsis'].map({\"None\": 0, \"hypermotile\": 1, 'distend_small':0, \"normal\": 2,\"hypomotile\": 3, \"absent\": 4}).astype(np.int8)\n        df['abdominal_distention'] = df['abdominal_distention'].map({\"None\": 0, \"none\": 1, \"slight\": 2, \"moderate\": 3, \"severe\": 4}).astype(np.int8)\n        df['nasogastric_tube'] = df['nasogastric_tube'].map({\"None\": 0, \"none\": 1, \"slight\": 2, \"significant\": 3}).astype(np.int8)\n        ## 'slight':0\n        df['nasogastric_reflux'] = df['nasogastric_reflux'].map({\"None\": 0, \"none\": 1, 'slight':0, \"less_1_liter\": 2, \"more_1_liter\": 3}).astype(np.int8)\n        ##  'serosanguious':0\n        df['rectal_exam_feces'] = df['rectal_exam_feces'].map({\"None\": 0, \"normal\": 1, \"increased\": 2, \"decreased\": 3, \"absent\": 4, 'serosanguious':0}).astype(np.int8)\n        df['abdomen'] = df['abdomen'].map({\"None\":0, \"normal\": 1, \"other\": 2, \"firm\": 3, \"distend_small\": 4, \"distend_large\": 5}).astype(np.int8)\n        df['abdomo_appearance'] = df['abdomo_appearance'].map({\"None\":0, \"clear\": 1, \"cloudy\": 2, \"serosanguious\": 3}).astype(np.int8)\n        df['surgical_lesion'] =df['surgical_lesion'].map({\"yes\": 1, \"no\": 0}).astype(np.int8)\n        df['cp_data'] = df['cp_data'].map({\"yes\": 1, \"no\": 0}).astype(np.int8)\n        \n#         #  Encoding the lesion 2 as 0/ non-zero:-\n#         df['lesion_2'] = df['lesion_2'].clip(0, 1).astype(np.int8)\n        \n        df[['hospital_number', 'lesion_1','lesion_2','lesion_3']] = df[['hospital_number', 'lesion_1','lesion_2','lesion_3']].astype(int)\n        self.op_cols = df.columns; \n        return df;\n    \n    def get_feature_names_in(self, X, y=None): \n        return self.ip_cols;    \n    \n    def get_feature_names_out(self, X, y=None): \n        return self.op_cols;","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:07.929042Z","iopub.execute_input":"2023-09-21T09:20:07.929529Z","iopub.status.idle":"2023-09-21T09:20:07.963267Z","shell.execute_reply.started":"2023-09-21T09:20:07.929489Z","shell.execute_reply":"2023-09-21T09:20:07.962076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#方式1\nsteps1 = [\n         (\"Imputer\", ColumnTransformer([\n                                         (\"num_\",\n                                          Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler()),]),\n                                          ['rectal_temp','pulse','respiratory_rate','nasogastric_reflux_ph','packed_cell_volume','total_protein','abdomo_protein']),\n                                        ],\n                                          remainder = SimpleImputer(strategy = \"most_frequent\"),verbose_feature_names_out = False\n                                       )),\n         ('CatEncoder', CatEncoder()),\n        ]\ndataPreprocess1 = Pipeline(steps = steps1, verbose = False)\nPrintColor(f\"\\ndataPreprocess1 pipeline structure\", color = Fore.BLUE)\ndisplay(dataPreprocess1)\ntrain_dataPreprocess1 = [Xtrain_dataPreprocess1,ytrain_dataPreprocess1] = dataPreprocess1.fit_transform(train_df.drop(target, axis=1, errors = 'ignore'), train_df[target].map(target_map).astype(np.int8)), train_df[target].map(target_map).astype(np.int8)\noriginal_dataPreprocess1 = [Xoriginal_dataPreprocess1,yoriginal_dataPreprocess1] = dataPreprocess1.fit_transform(original_df.drop(target, axis=1, errors = 'ignore'), original_df[target].map(target_map).astype(np.int8)), original_df[target].map(target_map).astype(np.int8)\ntrainANDoriginal_dataPreprocess1 = [XtrainANDoriginal_dataPreprocess1,ytrainANDoriginal_dataPreprocess1] = dataPreprocess1.fit_transform(trainANDoriginal_df.drop(target, axis=1, errors = 'ignore'), trainANDoriginal_df[target].map(target_map).astype(np.int8)), trainANDoriginal_df[target].map(target_map).astype(np.int8)\nXtest_dataPreprocess1= dataPreprocess1.transform(test_df)\nPrintColor(f\"\\nXtrain_dataPreprocess1 set Information\", color = Fore.BLUE)\ndisplay(Xtrain_dataPreprocess1.head(5).style.format(precision = 3))\nPrintColor(f\"\\nXoriginal_dataPreprocess1 set Information\", color = Fore.BLUE)\ndisplay(Xoriginal_dataPreprocess1.head(5).style.format(precision = 3))\nPrintColor(f\"\\nXtrainANDoriginal_dataPreprocess1 set Information\", color = Fore.BLUE)\ndisplay(XtrainANDoriginal_dataPreprocess1.head(5).style.format(precision = 3))\nPrintColor(f\"\\nXtest_dataPreprocess1 set Information\", color = Fore.BLUE)\ndisplay(Xtest_dataPreprocess1.head(5).style.format(precision = 3))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:06.346316Z","iopub.execute_input":"2023-09-21T09:27:06.346837Z","iopub.status.idle":"2023-09-21T09:27:06.668862Z","shell.execute_reply.started":"2023-09-21T09:27:06.346799Z","shell.execute_reply":"2023-09-21T09:27:06.667385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 训练数据选择\nXtrain,ytrain = Xtrain_dataPreprocess1,ytrain_dataPreprocess1\nXoriginal,yoriginal = Xoriginal_dataPreprocess1,yoriginal_dataPreprocess1\nXtrainANDoriginal,ytrainANDoriginal = XtrainANDoriginal_dataPreprocess1,ytrainANDoriginal_dataPreprocess1\nXtest= Xtest_dataPreprocess1","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:09.764129Z","iopub.execute_input":"2023-09-21T09:27:09.764813Z","iopub.status.idle":"2023-09-21T09:27:09.770939Z","shell.execute_reply.started":"2023-09-21T09:27:09.764777Z","shell.execute_reply":"2023-09-21T09:27:09.769804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipline后分析","metadata":{}},{"cell_type":"code","source":"## 信息增益，互信息\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    plt.figure(dpi=100, figsize=(8, 5))\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \nmi_scores = make_mi_scores(Xtrain,ytrain)\n# display(mi_scores[::3]) # show a few features with their MI scores\nplot_mi_scores(mi_scores)\nmi_scores = make_mi_scores(Xoriginal,yoriginal)\n# display(mi_scores[::3]) # show a few features with their MI scores\nplot_mi_scores(mi_scores)\nmi_scores = make_mi_scores(XtrainANDoriginal,ytrainANDoriginal)\n# display(mi_scores[::3]) # show a few features with their MI scores\nplot_mi_scores(mi_scores)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:11.272063Z","iopub.execute_input":"2023-09-21T09:27:11.272590Z","iopub.status.idle":"2023-09-21T09:27:14.550511Z","shell.execute_reply.started":"2023-09-21T09:27:11.272554Z","shell.execute_reply":"2023-09-21T09:27:14.549584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA分析","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 10)\nX2D = pca.fit_transform(Xtrain)\ndisplay(X2D)\ndisplay(pd.DataFrame(pca.explained_variance_ratio_).transpose().head(5).style.format(precision = 15))\nX2D = pca.fit_transform(Xoriginal)\ndisplay(X2D)\ndisplay(pd.DataFrame(pca.explained_variance_ratio_).transpose().head(5).style.format(precision = 15))\nX2D = pca.fit_transform(XtrainANDoriginal)\ndisplay(X2D)\ndisplay(pd.DataFrame(pca.explained_variance_ratio_).transpose().head(5).style.format(precision = 15))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:19.502664Z","iopub.execute_input":"2023-09-21T09:27:19.503057Z","iopub.status.idle":"2023-09-21T09:27:19.824107Z","shell.execute_reply.started":"2023-09-21T09:27:19.503029Z","shell.execute_reply":"2023-09-21T09:27:19.822094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 网格搜索相关","metadata":{}},{"cell_type":"code","source":"# Mdl_Master = \\\n# {'CBC': CatBoostClassifier(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n#                               'objective'           : \"MultiClass\",\n#                               'eval_metric'         : \"Accuracy\",\n#                               'classes_count'       : 3,\n#                               'bagging_temperature' : 0.10,\n#                               'colsample_bylevel'   : 0.75,\n#                               'iterations'          : 1000,\n#                               'learning_rate'       : 0.075,\n#                               'od_wait'             : 3,\n#                               'max_depth'           : 4,\n#                               'l2_leaf_reg'         : 0.85,\n#                               'min_data_in_leaf'    : 6,\n#                               'random_strength'     : 0.65, \n#                               'max_bin'             : 80,\n#                               'verbose'             : 0,\n#                               'use_best_model'      : True,\n#                            }\n#                          ), \n\n#   'LGBMC': LGBMClassifier(**{'device'            : \"gpu\" if CFG.gpu_switch == \"ON\" else \"cpu\",\n#                              'objective'         : 'multiclass',\n#                              'metric'            : 'none',\n#                              'boosting_type'     : 'gbdt',\n#                              'random_state'      : CFG.state,\n#                              'colsample_bytree'  : 0.5,\n#                              'subsample'         : 0.65,\n#                              'learning_rate'     : 0.08,\n#                              'max_depth'         : 4,\n#                              'n_estimators'      : 1000,\n#                              'num_leaves'        : 72,                    \n#                              'reg_alpha'         : 0.01,\n#                              'reg_lambda'        : 1.75,\n#                              'verbose'           : -1,\n#                          }\n#                       ),\n\n#   'XGBC': XGBClassifier(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n#                            'objective'          : 'multi:softprob',\n#                            'random_state'       : CFG.state,\n#                            'colsample_bytree'   : 0.7,\n#                            'learning_rate'      : 0.07,\n#                            'max_depth'          : 4,\n#                            'n_estimators'       : 1100,                         \n#                            'reg_alpha'          : 0.025,\n#                            'reg_lambda'         : 1.75,\n#                            'min_child_weight'   : 5,\n#                            'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n#                         }\n#                        ),\n \n#    'RFC' : RFC(n_estimators     = 150, \n#                criterion        = 'gini',\n#                max_depth        = 4,\n#                min_samples_leaf = 5,\n#                max_features     = 'log2',\n#                bootstrap        = True,\n#                oob_score        = True,\n#                random_state     = CFG.state,\n#                verbose          =0,\n#               ), \n \n#   \"HGBC\" : HGBC(loss              = 'categorical_crossentropy',\n#                 learning_rate     = 0.075,\n#                 early_stopping    = True,\n#                 max_iter          = 200,\n#                 max_depth         = 4,\n#                 min_samples_leaf  = 5,\n#                 l2_regularization = 1.75,\n#                 scoring           = myscorer,\n#                 random_state      = CFG.state,\n#                )\n# }","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:22.971041Z","iopub.execute_input":"2023-09-21T09:27:22.971521Z","iopub.status.idle":"2023-09-21T09:27:22.980092Z","shell.execute_reply.started":"2023-09-21T09:27:22.971485Z","shell.execute_reply":"2023-09-21T09:27:22.979081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 单模型----> 随机森林、LightGBM、XGBoost、CatBoost","metadata":{}},{"cell_type":"code","source":"Models=\\\n{\n    'RFC':{'model':RandomForestClassifier(n_estimators     = 150, ##\n                                          criterion        = 'gini',\n                                          max_depth        = 4, \n                                          min_samples_leaf = 5,\n                                          min_samples_split= 2,\n                                          max_features     = 'log2',\n                                          bootstrap        = True,\n                                          oob_score        = True,\n                                          random_state     = random_state,\n                                          verbose          = 0,),\n           'parameter_space':{\"n_estimators\": [79, 80, 81,150], \n                              \"min_samples_leaf\": [5, 29, 30, 31],\n                              \"min_samples_split\": [2, 3],\n                              \"max_depth\": [4,9, 10],\n                            }}\n}\n\nclf = Models['RFC']['model']\nparameter_space = Models['RFC']['parameter_space']\ngrid = GridSearchCV(clf, parameter_space, cv=2, scoring=\"neg_mean_squared_error\")\ngrid.fit(Xtrain.values, ytrain.values)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:27:25.513695Z","iopub.execute_input":"2023-09-21T09:27:25.514414Z","iopub.status.idle":"2023-09-21T09:28:26.716088Z","shell.execute_reply.started":"2023-09-21T09:27:25.514364Z","shell.execute_reply":"2023-09-21T09:28:26.715196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_\ngrid.best_estimator_\nnp.sqrt(-grid.best_score_)\ngrid.best_estimator_.predict(Xtest)\nresult = submission_df.copy()\nresult[target] = result[target].map(target_map).astype(np.int8)\nresult[target] = pd.Series(grid.best_estimator_.predict(Xtest)).map({k: v for k, v in zip(target_map.values(), target_map.keys())}).values\ndisplay(result)\nresult.to_csv(\"Submission_RFC.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:34:57.356066Z","iopub.execute_input":"2023-09-21T09:34:57.356619Z","iopub.status.idle":"2023-09-21T09:34:57.426489Z","shell.execute_reply.started":"2023-09-21T09:34:57.356581Z","shell.execute_reply":"2023-09-21T09:34:57.425032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 均值融合、加权融合、Stacking融合(分类问题用逻辑回归、否则用贝叶斯回归(bayesianRidge))","metadata":{}},{"cell_type":"code","source":"## trick","metadata":{"execution":{"iopub.status.busy":"2023-09-21T09:20:08.383232Z","iopub.status.idle":"2023-09-21T09:20:08.383922Z","shell.execute_reply.started":"2023-09-21T09:20:08.383613Z","shell.execute_reply":"2023-09-21T09:20:08.383640Z"},"trusted":true},"execution_count":null,"outputs":[]}]}